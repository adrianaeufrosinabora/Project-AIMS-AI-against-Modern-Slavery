{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install requests\n",
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download \n",
    "\n",
    "You need to download 'company_document_issue_responses.csv' file from https://www.business-humanrights.org/en/from-us/modern-slavery-statements/ by selecting **Download company response documents comparison data** button.\n",
    "\n",
    "Once the data csv file is downloaded, you can fetch the file in either pdf, png or jpg format by looking at  **Document Cached URL** column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company = pd.read_csv('company_document_issue_responses.csv')\n",
    "df_company['Document Cached URL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterrate each row in the dataframe and download the corresponding data from the provided url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download_file(url, local_file_location):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    if not os.path.isfile(local_file_location):\n",
    "        with open(local_file_location, 'wb') as f:\n",
    "            f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_FILE_FOLDER = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for _, row in df_company.iterrows():\n",
    "    url = row['Document Cached URL']\n",
    "    # use the last part of the url as the local file name\n",
    "    file_name = join(LOCAL_FILE_FOLDER, url.split('/')[-1])\n",
    "    download_file(url, file_name)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    #print('downloaded', file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from PDF and HTML\n",
    "\n",
    "The following code provides an example of extracting data from PDF files using a Python library called pdfplumber. However, you can choose any library as you prefer. \n",
    "\n",
    "You would need to do your own research if you wish to extract textual data from image file format such as png or jpg. However, the basic idea is to OCR the image file and then extract the text from the OCRed file.\n",
    "\n",
    "Note: there are scanned PDF files that should be treated as image files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import jsonpickle\n",
    "import pathlib\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from boilerpy3 import extractors\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def accepted_tags(element):\n",
    "    TAGS = ['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',\n",
    "            'section', 'a', 'a href', 'href', 'ul']\n",
    "    \n",
    "    if element.name in TAGS:\n",
    "        return True\n",
    "\n",
    "    elif element.parent.name in TAGS:\n",
    "        return True\n",
    "  \n",
    "    elif isinstance(element, Comment):\n",
    "        return False\n",
    "    \n",
    "    elif len(element)== 0:\n",
    "        return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "def processing_extracted_text(input_slice):\n",
    "    list_sentences = []\n",
    "    list_tags = ['h2', 'h3', 'h4']\n",
    "    for text_element in input_slice:\n",
    "        if text_element.parent.name == 'p' and text_element.name == 'a':\n",
    "\n",
    "                if \"href\" in str(text_element) and 'slavery' in str(text_element).lower():\n",
    "                    if text_element.text+\" \"+text_element['href'] not in list_sentences:\n",
    "                        if text_element.text+\" \"+text_element['href'].strip():\n",
    "                            list_sentences.append(text_element.text+\" \"+text_element['href'])\n",
    "                else:\n",
    "                    continue\n",
    "        elif text_element.name == 'a':\n",
    "            if \"href\" in str(text_element) and 'slavery' in str(text_element).lower():\n",
    "                if text_element.text+\" \"+text_element['href'] not in list_sentences:\n",
    "                    if text_element.text+\" \"+text_element['href'].strip():\n",
    "                        list_sentences.append(text_element.text+\" \"+text_element['href'])\n",
    "        try:\n",
    "            if text_element.name == 'p' and text_element['class'] == 'copyright':\n",
    "                continue\n",
    "        except (AttributeError, KeyError):\n",
    "            pass\n",
    "\n",
    "        if text_element.parent.name == 'a' and text_element.name == 'p':\n",
    "            continue\n",
    "        if text_element.parent.name == 'a' and text_element.name == 'span':\n",
    "            continue\n",
    "        if text_element.parent.name == 'nav' and text_element.name in list_tags:\n",
    "            continue\n",
    "        if \"email\" in text_element.text.lower():\n",
    "            continue\n",
    "        if \"a href\" not in str(text_element) and text_element.name != 'a':\n",
    "    \n",
    "            splitted_text_element = text_element.text.split()\n",
    "            cleaned_element = ' '.join(splitted_text_element)\n",
    "            if cleaned_element not in list_sentences:\n",
    "                list_sentences.append(cleaned_element)\n",
    "    return list_sentences\n",
    "\n",
    "\n",
    "def filter_tags(body):\n",
    "    soup  = BeautifulSoup(body, \"html.parser\")\n",
    "    texts = soup.findAll(name = ['p','a', 'h1', 'h2', 'h3','h4','li', 'ul'])\n",
    "    accepted_headers = [ 'h1', 'h2']\n",
    "    extracted_data1 = list(filter(accepted_tags, texts))\n",
    "    extracted_data = list(dict.fromkeys(filter(None, extracted_data1)))\n",
    "    list_sentences = []\n",
    "\n",
    "    for i, text_element in enumerate(extracted_data):\n",
    "        \n",
    "        if text_element.name in accepted_headers and 'slavery' in str(text_element).lower():\n",
    "        \n",
    "            list_sentences = processing_extracted_text(extracted_data[i:])\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            list_sentences = processing_extracted_text(extracted_data)\n",
    "            \n",
    "            \n",
    "    return \"\\n\".join(list_sentences)\n",
    "\n",
    "\n",
    "def extract_filtered_data(html_file):\n",
    "    list_invalid_pages = [\"page was not found\", \"url was not found\", \"page canâ€™t be found\",\"page isn't real\", \n",
    "    \"page is not real\", \"page not found\", \"error 404\", \"couldn't find the page\"]\n",
    "\n",
    "    with open (html_file, encoding='utf8',errors='replace') as f:\n",
    "        file_contents = f.read()\n",
    "\n",
    "        file_contents_without_metadata = filter_tags(file_contents).split(\"\\n\")\n",
    "        for sentence in file_contents_without_metadata:\n",
    "            if len(sentence.split())<=2:\n",
    "                file_contents_without_metadata.remove(sentence)\n",
    "\n",
    "        final_content = \"\\n\".join(file_contents_without_metadata)\n",
    "        \n",
    "        if any(element in final_content.lower() for element in list_invalid_pages):\n",
    "            return \"No Modern Slavery statement found\"\n",
    "                \n",
    "        elif len(final_content)==0:\n",
    "            return \"Nothing to return\"\n",
    "        \n",
    "        else:\n",
    "            return final_content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pdfplumber\n",
    "\n",
    "df_data = pd.DataFrame()\n",
    "for _, row in df_company.iterrows():\n",
    "    url = row['Document Cached URL']\n",
    "    file_name = join(LOCAL_FILE_FOLDER, url.split('/')[-1])\n",
    "    if os.path.isfile(file_name):\n",
    "        all_text = ''\n",
    "        try:\n",
    "            if file_name.endswith('pdf'):\n",
    "                with pdfplumber.open(file_name) as pdf:\n",
    "                    for pdf_page in pdf.pages:\n",
    "                        single_page_text = pdf_page.extract_text()\n",
    "                        if single_page_text:\n",
    "                            all_text = all_text + '\\n' + single_page_text\n",
    "            elif file_name.endswith('html') :\n",
    "                all_text = extract_filtered_data(file_name)\n",
    "        except:\n",
    "            print(file_name)\n",
    "            pass\n",
    "        row['extracted_text'] = all_text\n",
    "        df_data = df_data.append(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data[200:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
